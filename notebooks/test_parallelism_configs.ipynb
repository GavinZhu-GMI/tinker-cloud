{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelism Configuration Test\n",
    "\n",
    "Test different parallelism configurations (TP, CP, DP) with math_rl training and compare metrics.\n",
    "\n",
    "## Contents\n",
    "1. [Configuration](#configuration)\n",
    "2. [Helper Functions](#helper-functions)\n",
    "3. [Config 1: DP=4, TP=1, CP=1 (Baseline)](#config-1-dp4-tp1-cp1-baseline)\n",
    "4. [Config 2: DP=2, TP=2, CP=1](#config-2-dp2-tp2-cp1)\n",
    "5. [Config 3: DP=1, TP=2, CP=2](#config-3-dp1-tp2-cp2)\n",
    "6. [Results Comparison](#results-comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Training Configuration\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B\"\n",
    "GROUP_SIZE = 4\n",
    "GROUPS_PER_BATCH = 100\n",
    "LEARNING_RATE = 1e-4\n",
    "N_BATCHES = 10\n",
    "\n",
    "# Wandb Configuration\n",
    "WANDB_PROJECT = \"parallelism-test\"\n",
    "WANDB_API_KEY = \"0ed1fa8a77196635759510132f81ea55ced801bd\"\n",
    "\n",
    "# API Configuration\n",
    "API_BASE_URL = \"http://localhost:8000\"\n",
    "API_KEY = \"slime-dev-key\"\n",
    "\n",
    "# Paths\n",
    "OUTPUT_DIR = Path(\"/tmp/parallelism_test\")\n",
    "LOG_FILE = Path(\"/data/logs/tinkercloud.log\")\n",
    "OPENTINKER_DIR = Path(\"/root/gavin/tinkercloud\")\n",
    "\n",
    "# Environment\n",
    "os.environ[\"TINKER_API_KEY\"] = API_KEY\n",
    "os.environ[\"TINKER_BASE_URL\"] = API_BASE_URL\n",
    "os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
    "os.environ[\"PYTHONPATH\"] = f\"/root/gavin/tinkercloud:/root/Megatron-LM:/root/gavin/miles:{os.environ.get('PYTHONPATH', '')}\"\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Group Size: {GROUP_SIZE}\")\n",
    "print(f\"  Groups per Batch: {GROUPS_PER_BATCH}\")\n",
    "print(f\"  N Batches: {N_BATCHES}\")\n",
    "print(f\"  Wandb Project: {WANDB_PROJECT}\")\n",
    "print(f\"  Output Dir: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cmd(cmd, check=True, capture=True, cwd=None):\n",
    "    \"\"\"Run a shell command and return output\"\"\"\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=capture, text=True, cwd=cwd)\n",
    "    if capture:\n",
    "        if result.returncode != 0 and check:\n",
    "            print(f\"Error: {result.stderr}\")\n",
    "        return result.stdout.strip(), result.stderr.strip(), result.returncode\n",
    "    return None, None, result.returncode\n",
    "\n",
    "\n",
    "def wait_for_api(timeout=60):\n",
    "    \"\"\"Wait for API to be ready\"\"\"\n",
    "    start = time.time()\n",
    "    while time.time() - start < timeout:\n",
    "        try:\n",
    "            resp = requests.get(\n",
    "                f\"{API_BASE_URL}/health\",\n",
    "                headers={\"X-API-Key\": API_KEY},\n",
    "                timeout=5\n",
    "            )\n",
    "            if resp.status_code == 200 and resp.json().get(\"status\") == \"healthy\":\n",
    "                return True\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep(2)\n",
    "    return False\n",
    "\n",
    "\n",
    "def restart_server(tp: int, cp: int):\n",
    "    \"\"\"Restart tinkercloud server with new parallelism config\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Restarting server with TP={tp}, CP={cp}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Kill existing server\n",
    "    run_cmd(\"pkill -9 -f 'uvicorn.*training' 2>/dev/null || true\", check=False)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Clear log\n",
    "    LOG_FILE.write_text(\"\")\n",
    "    \n",
    "    # Start server with new parallelism\n",
    "    cmd = f\"\"\"\n",
    "    cd {OPENTINKER_DIR} && \\\n",
    "    SLIME_DEFAULT_TP={tp} SLIME_DEFAULT_CP={cp} ALLOW_PARTIAL_BATCHES=true \\\n",
    "    nohup python3 -m uvicorn training.api:app --host 0.0.0.0 --port 8000 \\\n",
    "    >> {LOG_FILE} 2>&1 &\n",
    "    \"\"\"\n",
    "    run_cmd(cmd, check=False)\n",
    "    \n",
    "    print(\"Waiting for server to start...\")\n",
    "    if wait_for_api(timeout=30):\n",
    "        print(\"Server is healthy\")\n",
    "    else:\n",
    "        print(\"ERROR: Server failed to start\")\n",
    "        print(LOG_FILE.read_text()[-2000:])\n",
    "        raise RuntimeError(\"Server failed to start\")\n",
    "    \n",
    "    # Cleanup existing sessions\n",
    "    print(\"Cleaning up existing sessions...\")\n",
    "    run_cmd(\"python /root/gavin/tinker_gmi/tests_integration/cleanup_test_env.py 2>/dev/null || true\", check=False)\n",
    "\n",
    "\n",
    "def run_training(config_name: str, tp: int, cp: int, dp: int):\n",
    "    \"\"\"Run training with specified parallelism config\"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    log_path = OUTPUT_DIR / config_name\n",
    "    wandb_name = f\"{config_name}-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running training: {config_name} ({N_BATCHES} batches)\")\n",
    "    print(f\"  TP={tp}, CP={cp}, DP={dp}\")\n",
    "    print(f\"  Log path: {log_path}\")\n",
    "    print(f\"  Wandb name: {wandb_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Remove old log dir\n",
    "    if log_path.exists():\n",
    "        import shutil\n",
    "        shutil.rmtree(log_path)\n",
    "    \n",
    "    # Run training\n",
    "    cmd = f\"\"\"\n",
    "    python -m tinker_cookbook.recipes.math_rl.train \\\n",
    "        model_name=\"{MODEL_NAME}\" \\\n",
    "        group_size={GROUP_SIZE} \\\n",
    "        groups_per_batch={GROUPS_PER_BATCH} \\\n",
    "        learning_rate={LEARNING_RATE} \\\n",
    "        n_batches={N_BATCHES} \\\n",
    "        log_path=\"{log_path}\" \\\n",
    "        wandb_project=\"{WANDB_PROJECT}\" \\\n",
    "        wandb_name=\"{wandb_name}\" \\\n",
    "        behavior_if_log_dir_exists=delete\n",
    "    \"\"\"\n",
    "    \n",
    "    stdout, stderr, rc = run_cmd(cmd, check=False)\n",
    "    if stdout:\n",
    "        print(stdout[-3000:])  # Last 3000 chars\n",
    "    if rc != 0:\n",
    "        print(f\"\\nTraining failed with exit code {rc}\")\n",
    "        if stderr:\n",
    "            print(stderr[-1000:])\n",
    "    \n",
    "    return log_path\n",
    "\n",
    "\n",
    "def get_final_metrics(log_path: Path) -> dict:\n",
    "    \"\"\"Extract final metrics from training log\"\"\"\n",
    "    metrics_file = log_path / \"metrics.jsonl\"\n",
    "    if not metrics_file.exists():\n",
    "        return {}\n",
    "    \n",
    "    # Read last line\n",
    "    lines = metrics_file.read_text().strip().split(\"\\n\")\n",
    "    if not lines:\n",
    "        return {}\n",
    "    \n",
    "    return json.loads(lines[-1])\n",
    "\n",
    "\n",
    "def print_metrics(config_name: str, metrics: dict):\n",
    "    \"\"\"Print formatted metrics\"\"\"\n",
    "    step = metrics.get('progress/batch', '-')\n",
    "    correct = metrics.get('env/all/correct', 0)\n",
    "    reward = metrics.get('env/all/reward/total', 0)\n",
    "    train_time = metrics.get('time/train', 0)\n",
    "    \n",
    "    print(f\"\\nFinal metrics for {config_name}:\")\n",
    "    print(f\"  Step: {step}\")\n",
    "    print(f\"  Correct: {correct:.3f}\")\n",
    "    print(f\"  Reward: {reward:.3f}\")\n",
    "    print(f\"  Train Time: {train_time:.1f}s\")\n",
    "\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Config 1: DP=4, TP=1, CP=1 (Baseline)\n",
    "\n",
    "Pure data parallelism - each GPU processes different samples independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config 1: DP=4, TP=1, CP=1 (baseline)\n",
    "CONFIG_1 = \"dp4-tp1-cp1\"\n",
    "TP_1, CP_1, DP_1 = 1, 1, 4\n",
    "\n",
    "restart_server(tp=TP_1, cp=CP_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path_1 = run_training(CONFIG_1, TP_1, CP_1, DP_1)\n",
    "metrics_1 = get_final_metrics(log_path_1)\n",
    "print_metrics(CONFIG_1, metrics_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Config 2: DP=2, TP=2, CP=1\n",
    "\n",
    "Tensor parallelism splits model weights across 2 GPUs, with 2-way data parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config 2: DP=2, TP=2, CP=1\n",
    "CONFIG_2 = \"dp2-tp2-cp1\"\n",
    "TP_2, CP_2, DP_2 = 2, 1, 2\n",
    "\n",
    "restart_server(tp=TP_2, cp=CP_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path_2 = run_training(CONFIG_2, TP_2, CP_2, DP_2)\n",
    "metrics_2 = get_final_metrics(log_path_2)\n",
    "print_metrics(CONFIG_2, metrics_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Config 3: DP=1, TP=2, CP=2\n",
    "\n",
    "Tensor + Context parallelism. TP=2 splits weights, CP=2 splits sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config 3: DP=1, TP=2, CP=2\n",
    "CONFIG_3 = \"dp1-tp2-cp2\"\n",
    "TP_3, CP_3, DP_3 = 2, 2, 1\n",
    "\n",
    "restart_server(tp=TP_3, cp=CP_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path_3 = run_training(CONFIG_3, TP_3, CP_3, DP_3)\n",
    "metrics_3 = get_final_metrics(log_path_3)\n",
    "print_metrics(CONFIG_3, metrics_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Results Comparison\n",
    "\n",
    "Compare all three configurations side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "configs = [\n",
    "    (\"dp4-tp1-cp1\", 1, 1, 4),\n",
    "    (\"dp2-tp2-cp1\", 2, 1, 2),\n",
    "    (\"dp1-tp2-cp2\", 2, 2, 1),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for config_name, tp, cp, dp in configs:\n",
    "    metrics = get_final_metrics(OUTPUT_DIR / config_name)\n",
    "    results.append({\n",
    "        \"config\": config_name,\n",
    "        \"tp\": tp,\n",
    "        \"cp\": cp,\n",
    "        \"dp\": dp,\n",
    "        \"correct\": metrics.get(\"env/all/correct\", \"N/A\"),\n",
    "        \"reward\": metrics.get(\"env/all/reward/total\", \"N/A\"),\n",
    "        \"train_time\": metrics.get(\"time/train\", \"N/A\"),\n",
    "    })\n",
    "\n",
    "# Print comparison table\n",
    "print(\"=\" * 70)\n",
    "print(\"PARALLELISM CONFIGURATION COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Config':<15} {'TP':>4} {'CP':>4} {'DP':>4} {'Correct':>10} {'Reward':>10} {'Time':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for r in results:\n",
    "    correct = f\"{r['correct']:.3f}\" if isinstance(r['correct'], float) else r['correct']\n",
    "    reward = f\"{r['reward']:.3f}\" if isinstance(r['reward'], float) else r['reward']\n",
    "    train_time = f\"{r['train_time']:.1f}s\" if isinstance(r['train_time'], float) else r['train_time']\n",
    "    \n",
    "    print(f\"{r['config']:<15} {r['tp']:>4} {r['cp']:>4} {r['dp']:>4} {correct:>10} {reward:>10} {train_time:>10}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "results_file = OUTPUT_DIR / \"comparison_results.json\"\n",
    "with open(results_file, \"w\") as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Results saved to: {results_file}\")\n",
    "print(f\"\\nWandb project: {WANDB_PROJECT}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Quick Reference\n",
    "\n",
    "### Parallelism Types\n",
    "\n",
    "| Type | Abbrev | What it splits | Communication |\n",
    "|------|--------|----------------|---------------|\n",
    "| Data Parallel | DP | Samples across GPUs | Gradient all-reduce |\n",
    "| Tensor Parallel | TP | Model weights (columns/rows) | Activation all-reduce |\n",
    "| Context Parallel | CP | Sequence length | Attention KV exchange |\n",
    "\n",
    "### GPU Allocation\n",
    "\n",
    "With 4 GPUs: `DP × TP × CP = 4`\n",
    "\n",
    "| Config | DP | TP | CP | Use Case |\n",
    "|--------|----|----|----|---------|\n",
    "| dp4-tp1-cp1 | 4 | 1 | 1 | Small models, max throughput |\n",
    "| dp2-tp2-cp1 | 2 | 2 | 1 | Medium models |\n",
    "| dp1-tp2-cp2 | 1 | 2 | 2 | Large models, long sequences |\n",
    "\n",
    "### Commands\n",
    "\n",
    "```bash\n",
    "# Set parallelism via environment\n",
    "SLIME_DEFAULT_TP=2 SLIME_DEFAULT_CP=2 python -m uvicorn training.api:app\n",
    "\n",
    "# Check server logs\n",
    "tail -f /data/logs/tinkercloud.log\n",
    "\n",
    "# View wandb results\n",
    "# https://wandb.ai/<entity>/parallelism-test\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
